0. Can you come up out 3 sceneraies which use AI methods?
Ans: Unmanned Vehicleï¼ŒFace Recognitionï¼Œalpha go.

1. How do we use Github; Why do we use Jupyter and Pycharm;
Ans: We use Pycharm to code. Jupyter Notebooks are a powerful way to write and iterate on your Python code for data analysis.
We use Git to work with other people and it becomes easier to collaborate on projects.

2. What's the Probability Model?
Ans: A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event.

3. Can you came up with some sceneraies at which we could use Probability Model?
Ans:Weather forecast,Stock Forecast

4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?
Ans:

5. What's the Language Model;
Ans: A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability to the whole sequence. The language model provides context to distinguish between words and phrases that sound similar.
ğ‘™ğ‘ğ‘›ğ‘”ğ‘¢ğ‘ğ‘”ğ‘’_ğ‘šğ‘œğ‘‘ğ‘’ğ‘™(ğ‘†ğ‘¡ğ‘Ÿğ‘–ğ‘›ğ‘”)=ğ‘ƒğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦(ğ‘†ğ‘¡ğ‘Ÿğ‘–ğ‘›ğ‘”)âˆˆ(0,1)

6. Can you came up with some sceneraies at which we could use Language Model?
Ans:Input Method, Taobao Customer Service and Translation Software

7. What's the 1-gram language model;
Ans:An 1-gram is a sequence of 1 words
ğ‘ƒğ‘Ÿğ‘œ(ğ‘¤1ğ‘¤2ğ‘¤3ğ‘¤4)=ğ‘ƒğ‘Ÿ(ğ‘¤1|ğ‘¤2ğ‘¤3ğ‘¤4)âˆ—ğ‘ƒ(ğ‘¤2|ğ‘¤3ğ‘¤4)âˆ—ğ‘ƒğ‘Ÿ(ğ‘¤3|ğ‘¤4)âˆ—ğ‘ƒğ‘Ÿ(ğ‘¤4)

8. What's the disadvantages and advantages of 1-gram language model;
Ans: Unigram language models are often smoothed to avoid instances where P(term) = 0. A common approach is to generate a maximum-likelihood model for the entire collection and linearly interpolate the collection model with a maximum-likelihood model for each document to smooth the model.

9. What't the 2-gram models;
Ans:The bigram model approximates the probability of a word given all the previous words by using only the conditional probability of the preceding word. 
ğ‘ƒğ‘Ÿğ‘œ(ğ‘¤1ğ‘¤2ğ‘¤3ğ‘¤4)âˆ¼ğ‘ƒğ‘Ÿ(ğ‘¤1|ğ‘¤2)âˆ—ğ‘ƒ(ğ‘¤2|ğ‘¤3)âˆ—ğ‘ƒğ‘Ÿ(ğ‘¤3|ğ‘¤4)âˆ—ğ‘ƒğ‘Ÿ(ğ‘¤4)